{"cells":[{"metadata":{"trusted":true,"_uuid":"e138b49f60e76e93a1f31f81af230f3be8534009"},"cell_type":"code","source":"####STEPS####\n#1-import library\n#2-insert dataset\n#3-head()\n#4-samplee()\n#5-tail()\n#6-shape\n#7-describe()\n#8-groupby()\n#9-hist()\n#10-corr()\n#11-correlation map\n#12-scatter Plot\n#13-isnull()\n#14-outliers\n#15-create new attribute\n#16-Normalization\n#17-Mulitple Lineer Regression\n#18-Naive Bayes\n#19-model results and interpretation\n#20-Interpration of results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/honeyproduction.csv')       ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"094e16c6d3580b430951d45ac04f6e74840773e5"},"cell_type":"code","source":"df.info()      #memory usage and data types","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31880c84b82f840910ef8c712937f1b39d95e1ba"},"cell_type":"code","source":"df.head(10)        #first 10 rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dedd5b68bbf0da36a3c3cfae1f491d28482469a"},"cell_type":"code","source":"df.sample(6)     #random 6 rows ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ad567accb7b689dd485f9968b5ee75a12f1c81a"},"cell_type":"code","source":"df.tail(10)      #last 10 rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6826ce1975ff93754b03580e209c719edad8587c"},"cell_type":"code","source":"df.shape          #dataset columns and lines ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f5624f97041b8eb3d07d19fd0ae4b0025fbfb56"},"cell_type":"code","source":"df.describe()           #important values in the data(min,max,count,std,%)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9408d4b6e57ed8c16d355ffd4b3b36c5c6e53f1"},"cell_type":"code","source":"df.groupby(\"year\").size()             ## How are class distributions?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4022e4126fa80289eb1e0cd535792a46ba5da54"},"cell_type":"code","source":"df.hist()         #sataset histogram review","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d034d617334f74488a934181e03644c9aa7c89a7"},"cell_type":"code","source":"df.corr()\n#In the data set, there is a good relationship between the corr and the correct proportions and inverse proportions.\n#Or: There is a directional link between stocks and totalprod.\n#Or: There is a very ineffective connection between priceperib and yieldpercol.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05859603e0fe7bf00a9b304da2d5230d823a78bf"},"cell_type":"code","source":"#correlation map\nf,ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(df.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()\n#The heat map showed that the color was open and there was a lateral connection.\n# When the color is placed, there is a reverse connection.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab51d2a771f60a1f9803c58d03f7ad1c6a8880ba"},"cell_type":"code","source":"#Scatter Plot\ndf.plot(kind='scatter', x='totalprod', y='prodvalue',alpha = 0.5,color = 'blue')\nplt.xlabel('totalprod')              # label = name of label\nplt.ylabel('prodvalue')\nplt.title('totalprod-prodvalue Scatter Plot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a74c887444697e8e8127c2566bd38303d026c87b"},"cell_type":"code","source":"df.isnull().sum()  #null value control\n                   #There is no missing value in the dataset ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a929445f7c3dc3d9096b87cb42515f543e1d89e1"},"cell_type":"code","source":"sns.boxplot(x=df['priceperlb'])                   #Value Value Detection\n                                                  #data gathered at a certain point\n                                                  #the data set is significantly negative","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51f9809ee512737ed29c7a606edeb03af9412d0a"},"cell_type":"code","source":"#Create New Attribute\nstockFilt = (df['stocks'])\ndf['stockFilt']=stockFilt/1000 \ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"829d149915a8ec1c53f22257085d92d2067bf4bc"},"cell_type":"code","source":"#Normalization\nfrom sklearn import preprocessing\nx = df[['totalprod']].values.astype(float)       #Normalize the 'totalprod' attribute\n\nmin_max_scaler = preprocessing.MinMaxScaler()    #We use the MinMax normalization method for normalization\nx_scaled = min_max_scaler.fit_transform(x)\ndf['totalprod-2'] =pd.DataFrame(x_scaled)        #New 'totalprod-2' column\n\ndf.head(12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cdef04151cf0042055406562ecabd09b19ee20c"},"cell_type":"code","source":"#Creating df2\ndf2=df[['numcol','yieldpercol','totalprod','stocks','priceperlb','prodvalue']]\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59882bbc1f55cee16ba17db474a899990f9c75d6"},"cell_type":"code","source":"#Creating df3\ndf3=df['year']\ndf3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77db0b6d6192116779a1b4173da9adb934a964bc"},"cell_type":"code","source":"X = df2.iloc[::].values                   #Select relevant attribute values for training\nY = df3.iloc[::].values                   #Select classification attribute values\nY","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48bf6e67ffb34696165d337161e1cd55916c11e0","_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"#½20 test and ½80 train(Multiple Lineer Regression)\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82db6950b43da0cb28ea340fd38e5cc9cec5c010"},"cell_type":"code","source":"#model import operation\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d81d5fa052d28d580df6bb263095d508b41051cd"},"cell_type":"code","source":"# Predicting the Test set results\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbef4bdce74726c3e407a35955aab98f2ed9851d"},"cell_type":"code","source":"#Results Mulitple Lineer Regression\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba644ab61ee5fda954dbdfdfb075c7b3f891f5f9"},"cell_type":"code","source":"#½30 test and ½70 train(Multiple Lineer Regression)\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n#Results Mulitple Lineer Regression\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4cb4a091f7afca38311f0101b62e11a2979d744"},"cell_type":"code","source":"#Results Multiple Lineer Regression\n#Mean Squared Error(MSE) is quite similar to Mean Absolute Error, the only difference being that MSE takes the average of the square of the difference between the original values and the predicted values\n#Mean Absolute Error is the average of the difference between the Original Values and the Predicted Values. It gives us the measure of how far the predictions were from the actual output\n#Confusion Matrix as the name suggests gives us a matrix as output and describes the complete performance of the model.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d87dc3940622485b32f371666b493a7b13352b2"},"cell_type":"code","source":"#Naive Bayes \nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn import model_selection\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdb7221f13655efb388a363cacacc554e469ddbd"},"cell_type":"code","source":"X2 = df2.iloc[::].values                 #Select relevant attribute values for training\nY2 = df3.iloc[::].values                 #Select classification attribute values\n\nX2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"158a89af1918f539c806b7dee6e1617c44da492e"},"cell_type":"code","source":"#test and train\nvalidation_size = 0.20\nseed = 7\nX_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X2, Y2, test_size=validation_size, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3920a364c774ffe158e4e269f81d06c36adda40c"},"cell_type":"code","source":"#Creation of Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df2ec3fa1864414c68cc84d8d309398f1985fb4b"},"cell_type":"code","source":"scoring = 'accuracy'\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\ncv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n\ncv_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"555fca3aee05a08fcc9da96335a47ed04e173edf"},"cell_type":"code","source":"msg = \"%f (%f)\" % (cv_results.mean(), cv_results.std())\nmsg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd133ed6653a77ec31f8e0a24bdb1362e239aafa"},"cell_type":"code","source":"from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X2, Y2, test_size = 0.2, random_state = 0)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"be011eb4dafee1a81600ead0b86611e022edb156"},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint(\"ACC: \",accuracy_score(y_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6724954b1abbf7b604a1ffb0d178470c9d01799b"},"cell_type":"code","source":"#NAIVE BAYES RESULTS\n#precision=The number of correct positive results is divided by the number of positive results estimated by the classifier.\n#low value of recall value\n#F1 Score tries to find the balance between precision and recall.The F1 score is low in Naive bayes\n#Recall :It is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive).\n#Recall rate efficiency is high only in 2001\n#Confusion Matrix as the name suggests gives us a matrix as output and describes the complete performance of the model.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca505627318e514e038b53b581649542c3f7c89f"},"cell_type":"code","source":"#INTERPRATATION OF RESULTS\n#We used 2 different model training in this data set.(Naive Bayes,Multiple Lineer Regression)\n#ACC value in model training is not high\n#Confusion Matrix values vary by years.\n#Other outcome measures differ slightly in test and train operations.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9482a0c540dc6328a30401f0458eb593ea22ceac"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}